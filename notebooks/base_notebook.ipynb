{"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["ArJdkJrLDSyG"],"gpuType":"T4","machine_shape":"hm","provenance":[]},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7652083,"sourceType":"datasetVersion","datasetId":4426389,"isSourceIdPinned":false}],"dockerImageVersionId":30648,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Import Libraries and Seeds Configuration","metadata":{"id":"AOQfVwh-BaSB"}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom scipy.ndimage import gaussian_filter, distance_transform_edt\nimport random\nimport os\nimport cv2\nimport tarfile\nfrom PIL import Image\n\nfrom tqdm import tqdm\nimport time\n\nfrom sklearn.model_selection import train_test_split\n\nimport torch\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\nfrom torchvision.transforms import Normalize, Resize, Compose\nfrom torchvision.transforms.functional import to_pil_image\nfrom torch.optim.lr_scheduler import ExponentialLR\n\nimport torch.nn as nn\nimport torch.nn.functional as F","metadata":{"id":"iJ-cUrihk_eD","tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"try:\n    from torchinfo import summary\nexcept:\n    print(\"[INFO] Couldn't find torchinfo... installing it.\")\n    !pip install -q torchinfo\n    from torchinfo import summary\n\ntry:\n    import rasterio\nexcept:\n    print(\"[INFO] Couldn't find rasterio... installing it.\")\n    !pip install rasterio\n    import rasterio","metadata":{"id":"-QaRVtDi2-qm","outputId":"dc99d510-d669-40b2-e50b-24fe8671b700","tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Set random seeds to ensure reproducibility\ntorch.manual_seed(44)\nnp.random.seed(44)\nrandom.seed(44)\nrandom_state = 44","metadata":{"id":"QWWY3ks8jth8","tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Loading/ Filtering\n","metadata":{"id":"tCMb4oogzYWS"}},{"cell_type":"code","source":"# Change only these three paths accordingly to run-environment\nraw_data = r'/kaggle/input/kelpsegmentationdataset/data/data/raw'\nprocessed_data = r'/kaggle/input/kelpsegmentationdataset/data/data/processed'\noutputs = r'/kaggle/working/'\n\n# Raw data\ntrain_satellite = os.path.join(raw_data, 'train_satellite')\ntrain_kelp = os.path.join(raw_data, 'train_kelp')\ntest_satellite = os.path.join(raw_data, 'test_satellite')\nmetadata_csv = os.path.join(raw_data, 'metadata_fTq0l2T.csv')\n\n# Processed data\ndata_statistics = os.path.join(processed_data, 'train_img_statistics.csv')\nnormalization_statistics_sb = os.path.join(processed_data, 'norm_stats_sb_low5_up5_perc_v2.npz')\nnormalization_statistics_dist= os.path.join(processed_data, 'norm_stats_dist_total.npz')\nnormalization_statistics_dem = os.path.join(processed_data, 'norm_stats_dem_total.npz')\nnormalization_statistics_ndvi = os.path.join(processed_data, 'norm_stats_ndvi_low5_up5_perc_v2.npz')\n\ntrain_satellite_np = os.path.join(processed_data, 'train_satellite_np')\ntrain_distance_maps_np = os.path.join(processed_data, 'train_distance_maps_np')\ntrain_ndvi_np = os.path.join(processed_data, 'train_ndvi_np')\ntrain_kelp_np = os.path.join(processed_data, 'train_kelp_np')\ntest_satellite_np = os.path.join(processed_data, 'test_satellite_np')\ntest_distance_maps_np =  os.path.join(processed_data, 'test_distance_maps_np')\ntest_ndvi_np = os.path.join(processed_data, 'test_ndvi_np')\n\n# Outputs\npredictions =  os.path.join(outputs, 'predictions')\ncheckpoints =  os.path.join(outputs, 'checkpoints')\n\n# Create necessary directories\nfor directory in [predictions, checkpoints]:\n    os.makedirs(directory, exist_ok=True)\n    print(f\"Created or verified the existence of the directory: {directory}\")","metadata":{"id":"_98cuZNvjurR","outputId":"21a4ab81-41e2-42f5-e15c-7fc103beddf5","tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Loading data and metadata files","metadata":{"id":"cBAQfSoa0y1U"}},{"cell_type":"code","source":"# Load metadata and statistics\nmetadata = pd.read_csv(metadata_csv)\ntrain_statistics = pd.read_csv(data_statistics)\n\n# Select metadata for train and test datasets\ntrain_metadata = metadata[metadata['in_train'] == True]\ntest_given_metadata = metadata[(metadata['in_train'] == False) & (metadata['type'] == 'satellite')]\n\n# Join metadata file with img statistics\ntrain_metadata_stats = pd.merge(train_metadata, train_statistics, right_on='image_id', left_on = 'tile_id', how='inner')\n\ntrain_metadata_stats.head()","metadata":{"id":"HJkPiY3C074n","outputId":"18d9181b-6c78-4004-e889-918a01b9d69d","tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Filter data according to statistics","metadata":{}},{"cell_type":"code","source":"# Get filtered dataset (to be splitted after into train, val, test)\nfiltered_train_metadata = train_metadata_stats[(train_metadata_stats['num_kelp_px'] > 0) & (train_metadata_stats['perc_clouds'] < 0.2) & (train_metadata_stats['perc_corrupt'] < 0.1)]\nprint(f'filtered_train_dataset: {filtered_train_metadata.shape}')\n\n# Get test_un_2 dataset (metadata of all the images not included in the filtered_train_metadata dataset)\ntest_un_2 = train_metadata_stats[(train_metadata_stats['num_kelp_px'] == 0) | (train_metadata_stats['perc_clouds'] > 0.2) | (train_metadata_stats['perc_corrupt'] > 0.1)]\nprint(f'test_un_2: {test_un_2.shape}')\n\nprint(f'sum: {len(test_un_2) + len(filtered_train_metadata)}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train-Test-Val Split","metadata":{"id":"9Ye2BxBgIC_d"}},{"cell_type":"code","source":"# Data Stratification\nbin_count = 20\nbin_numbers = pd.qcut(x=filtered_train_metadata['perc_kelp'], q=bin_count, labels=False, duplicates='drop')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define training way\n# run_on = 'sample' or 'train_split' or 'train_total'\nrun_on = 'train_split'\n\n# Selects only a subsample of the filtered dataset for debuging (ex: 5%) and splits into train, val, test\nif run_on == 'sample':\n  # Select subset of 5% of the data\n  subset_metadata, _ = train_test_split(filtered_train_metadata, test_size=0.95, random_state=random_state)\n  # Split the reduced dataset into train, val, test\n  train_metadata, temp_metadata = train_test_split(subset_metadata, test_size=0.3, random_state=random_state)\n  val_metadata, test_metadata = train_test_split(temp_metadata, test_size=0.5, random_state=random_state)\n\n# Uses the total filtered dataset and splits it into train, val, test\nelif run_on == 'train_split':\n  # Split train dataset into train, val, test\n  train_metadata, temp_metadata = train_test_split(filtered_train_metadata, test_size=960, random_state=random_state, stratify= bin_numbers)\n  bin_numbers_temp = pd.qcut(x=temp_metadata['perc_kelp'], q=bin_count, labels=False, duplicates='drop')  \n  val_metadata, test_metadata = train_test_split(temp_metadata, test_size=0.5, random_state=random_state, stratify= bin_numbers_temp)\n\n# Uses the total filtered dataset and divides into train and val and uses the given train test\nelif run_on == 'train_total':\n  # Split into train and val (10%)\n  train_metadata, val_metadata = train_test_split(filtered_train_metadata, test_size=640, random_state=random_state, stratify= bin_numbers)","metadata":{"id":"ZPmHAEwBIH7g","tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Conversion of from tif to npz","metadata":{"id":"ArJdkJrLDSyG","tags":[]}},{"cell_type":"markdown","source":"### Conversion of data\n","metadata":{"id":"jY5S16k5o37R"}},{"cell_type":"code","source":"def calculate_distance_from_coast(image, sigma=1, constant_distance=1000):\n\n    # Extract the DEM from the 7th layer of the image\n    dem = image[6, :, :]\n\n    # Smooth the DEM\n    dem_sm = gaussian_filter(dem, sigma=sigma)\n\n    # Binarize the smoothed DEM\n    dem_bw = dem_sm > 0.0\n\n    # Compute the Canny edges to find the coastline\n    edges_canny = cv2.Canny((dem_bw * 255).astype(np.uint8), 50, 150)\n\n    # Invert the edges to represent background (non-edge regions)\n    background_mask = ~edges_canny.astype(bool)\n\n    # Combine the binarized DEM and the background mask to exclude coastline edges\n    background_binarized_dem = dem_bw & background_mask\n\n    # If the whole image is water (or there's no coastline), set the distance map to a constant value\n    if np.sum(background_binarized_dem) == 0:\n        distance_map_background = np.ones_like(dem_bw) * constant_distance\n    else:\n        # Compute distance map for the background, indicating distance from coastline\n        distance_map_background = distance_transform_edt(~background_binarized_dem)\n    return distance_map_background","metadata":{"id":"HgrV_KXISvy0","tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def convert_tiff_to_npz(tif_files_path, original_data_target_path, distance_map_target_path=None, is_labels=False):\n#     # List all TIFF files in the source directory\n#     tiff_files = [f for f in os.listdir(tif_files_path) if f.endswith('.tif')]\n\n#     # Process each TIFF file\n#     for filename in tqdm(tiff_files, desc=\"Converting TIFF to NPZ\"):\n#         file_path = os.path.join(tif_files_path, filename)\n\n#         with rasterio.open(file_path) as src:\n#             image_array = src.read()  # Read the image as a NumPy array\n\n#             # Determine the correct data type based on whether the image is a label\n#             data_type = np.int8 if is_labels else np.int16\n\n#             # Convert the image array to the determined data type\n#             image_array = image_array.astype(data_type)\n\n#             # Save the original image data\n#             original_npz_filename = os.path.splitext(filename)[0] + '.npz'\n#             original_npz_file_path = os.path.join(original_data_target_path, original_npz_filename)\n#             np.savez_compressed(original_npz_file_path, image_array)\n\n#             # Only calculate and save distance maps for non-label images\n#             if not is_labels:\n#                 # Calculate the distance map, assume the function `calculate_distance_from_coast` is defined\n#                 distance_map = calculate_distance_from_coast(image_array)\n\n#                 # Save the distance map\n#                 distance_map_npz_filename = os.path.splitext(filename)[0] + '_distance_map.npz'\n#                 distance_map_npz_file_path = os.path.join(distance_map_target_path, distance_map_npz_filename)\n#                 np.savez_compressed(distance_map_npz_file_path, distance_map)\n\n# convert_tiff_to_npz(train_satellite, train_satellite_np, train_distance_maps_np, is_labels=False)\n# convert_tiff_to_npz(train_kelp, train_kelp_np, None, is_labels=True)\n# convert_tiff_to_npz(test_satellite, test_satellite_np, test_distance_maps_np, is_labels=False)","metadata":{"id":"qn1zQsSEgo2k","tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def count_and_size_npz_images(directory_path):\n    # List all files in the directory\n    files = os.listdir(directory_path)\n    # Initialize total size\n    total_size = 0\n    # Count files with the .npz extension and sum their sizes\n    npz_count = 0\n    for f in files:\n        if f.endswith('.npz'):\n            npz_count += 1\n            file_path = os.path.join(directory_path, f)\n            total_size += os.path.getsize(file_path)\n    # Convert total size from bytes to gigabytes\n    total_size_gb = total_size / (1024**3)\n    return npz_count, total_size_gb\n\n\n# Print the number of NPZ images and their total size in each directory\ncount, size = count_and_size_npz_images(train_satellite_np)\nprint(f\"Train Satellite Data: {count} files, {size:.2f} GB\")\n\ncount, size = count_and_size_npz_images(train_distance_maps_np)\nprint(f\"Train Distance Maps: {count} files, {size:.2f} GB\")\n\ncount, size = count_and_size_npz_images(train_ndvi_np)\nprint(f\"Train NDVI: {count} files, {size:.2f} GB\")\n\ncount, size = count_and_size_npz_images(train_kelp_np)\nprint(f\"Train Kelp Labels: {count} files, {size:.2f} GB\")\n\ncount, size = count_and_size_npz_images(test_satellite_np)\nprint(f\"Test Satellite Data: {count} files, {size:.2f} GB\")\n\ncount, size = count_and_size_npz_images(test_distance_maps_np)\nprint(f\"Test Distance Maps: {count} files, {size:.2f} GB\")\n\ncount, size = count_and_size_npz_images(test_ndvi_np)\nprint(f\"Test NDVI: {count} files, {size:.2f} GB\")","metadata":{"id":"6ws_FYjeCD0b","outputId":"e57389f9-fb90-425f-8d12-660c3cb282ed","tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Custom Dataset Class and Dataloader","metadata":{"id":"4aGxz7zHjZnL"}},{"cell_type":"code","source":"class KelpDataset(Dataset):\n    def __init__(self, metadata, data_path, distance_map_path=None, ndvi_path=None, label_path=None, data_transforms=None, label_transforms=None):\n        self.metadata = metadata\n        self.data_path = data_path\n        self.distance_map_path = distance_map_path\n        self.ndvi_path = ndvi_path\n        self.label_path = label_path\n        self.data_transforms = data_transforms\n        self.label_transforms = label_transforms\n\n    def __len__(self):\n        return len(self.metadata)\n\n    def __getitem__(self, idx):\n        # Load spectral bands and cloud masks\n        data_name = os.path.join(self.data_path, self.metadata.iloc[idx]['filename'].replace('.tif', '.npz'))\n        with np.load(data_name) as data_file:\n            data = data_file[data_file.files[0]]\n        spectral_bands = data[[0, 1, 2, 3, 4], :, :]\n        cloud_mask = np.expand_dims(data[5, :, :], axis=0)  # Shape is [1, height, width]\n        dem = np.expand_dims(data[6, :, :], axis=0)\n        \n        # Convert from Numpy Arrays to Torch Tensors\n        spectral_bands = torch.tensor(spectral_bands, dtype=torch.float32)\n        dem = torch.tensor(dem, dtype=torch.float32)\n        cloud_mask = torch.tensor(cloud_mask, dtype=torch.uint8)\n\n        # Load distance map\n        distance_map_name = os.path.join(self.distance_map_path, self.metadata.iloc[idx]['tile_id'] + '_satellite_distance_map.npz')\n        with np.load(distance_map_name) as distance_map_file:\n            distance_map = distance_map_file[distance_map_file.files[0]]\n        distance_map = np.expand_dims(distance_map, axis=-1)  # Shape is [height, width, 1]\n        distance_map = torch.tensor(distance_map, dtype=torch.float32).permute(2, 0, 1)  # Shape is [1, height, width]\n        \n        # Load ndvi\n        ndvi_name = os.path.join(self.ndvi_path, self.metadata.iloc[idx]['tile_id'] + '_satellite.npz')\n        with np.load(ndvi_name) as ndvi_file:\n            ndvi = ndvi_file[ndvi_file.files[0]]\n        ndvi = np.expand_dims(ndvi, axis=-1)  # Shape is [height, width, 1]\n        ndvi = torch.tensor(ndvi, dtype=torch.float32).permute(2, 0, 1)  # Shape is [1, height, width]\n        \n        # Load label\n        label = None\n        if self.label_path:\n            label_name = os.path.join(self.label_path, self.metadata.iloc[idx]['tile_id'] + '_kelp.npz')\n            if os.path.exists(label_name):\n                with np.load(label_name) as label_file:\n                    label = torch.tensor(label_file[label_file.files[0]], dtype=torch.uint8)\n        if label is None:\n            # Initialize label with zeros if not available\n            default_label_shape = (1, data.shape[1], data.shape[2])\n            label = torch.zeros(default_label_shape, dtype=torch.uint8)\n\n        # Stack spectral bands, distance map, dem & ndvi\n        data = torch.cat([spectral_bands, distance_map, dem, ndvi], dim=0)\n\n        # Apply transformations if provided\n        if self.data_transforms is not None:\n            data = self.data_transforms(data)\n        if self.label_transforms is not None and label is not None:\n            label = self.label_transforms(label)\n\n        return data, label, cloud_mask, self.metadata.iloc[idx]['tile_id']\n","metadata":{"id":"eYmbx2ZA5aLW","tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Normalization: Band Statistics for Training Set","metadata":{"id":"a9-okToyNwx9"}},{"cell_type":"code","source":"# Paths to your .npz files\nnormalization_npz_files = {\n    'sb': normalization_statistics_sb,\n    'dist': normalization_statistics_dist,\n    'dem': normalization_statistics_dem,\n    'ndvi': normalization_statistics_ndvi,\n}\n\n# Initialize lists to hold all means and stds\nall_means = []\nall_stds = []\n\n# Loop through each .npz file, load data, and extract means and stds\nfor key, file_path in normalization_npz_files.items():\n    data = np.load(file_path)\n    means = np.atleast_1d(data['means'])\n    stds = np.atleast_1d(data['stds'])\n\n    # Append to lists\n    all_means.append(means)\n    all_stds.append(stds)\n\n# Concatenate all means and stds\nall_means = np.concatenate(all_means, axis=0)\nall_stds = np.concatenate(all_stds, axis=0)\n\n# Display the concatenated means and stds\nprint(\"\\nConcatenated Means:\\n\", all_means)\nprint(\"\\nConcatenated Stds:\\n\", all_stds)","metadata":{"id":"hsuxPFfWkdbr","outputId":"383107ea-aef3-4c81-ee7f-7f77f35be2be","tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Transforms","metadata":{"id":"bnOUHrG-mVOb"}},{"cell_type":"code","source":"# Transformations\ndata_transforms = Compose([\n    Normalize(mean=all_means,\n              std=all_stds)])","metadata":{"id":"qmah3jBFk53U","tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Instantiate Datasets and Dataloaders","metadata":{"id":"1DWMNafQm1OS"}},{"cell_type":"code","source":"# Instantiate datasets\ntrain_dataset = KelpDataset(metadata=train_metadata, data_path=train_satellite_np, distance_map_path=train_distance_maps_np, ndvi_path=train_ndvi_np, label_path=train_kelp_np, data_transforms=data_transforms, label_transforms=None)\nval_dataset = KelpDataset(metadata=val_metadata, data_path=train_satellite_np, distance_map_path=train_distance_maps_np, ndvi_path=train_ndvi_np, label_path=train_kelp_np, data_transforms=data_transforms, label_transforms=None)\ntest_dataset = KelpDataset(metadata=test_metadata, data_path=train_satellite_np, distance_map_path=train_distance_maps_np, ndvi_path=train_ndvi_np, label_path=train_kelp_np, data_transforms=data_transforms, label_transforms=None)\ntest_given_dataset = KelpDataset(metadata=test_given_metadata, data_path=test_satellite_np, distance_map_path=test_distance_maps_np, ndvi_path=test_ndvi_np,  label_path=None, data_transforms=data_transforms, label_transforms=None)\n\n# Instantiate dataloaders\nbatch_size = 16\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\ntest_given_loader = DataLoader(test_given_dataset, batch_size=batch_size, shuffle=False)","metadata":{"id":"1IE6yUXViwRC","tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Datasets Inspection","metadata":{"id":"M1zS4YYoXMKg"}},{"cell_type":"code","source":"print(f\"Length of Training Dataset: {len(train_dataset)}\")\nprint(f\"Length of Validation Dataset: {len(val_dataset)}\")\nprint(f\"Length of Test Dataset: {len(test_dataset)}\")\nprint(f\"Length of Test Given Dataset: {len(test_given_dataset)}\")","metadata":{"id":"Nbe67Bv6Oq_V","outputId":"c6d2654c-868c-4366-b864-534bbc3c1113","tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Shapes and Types train Dataloader\nfor data, label, cloud_mask, _ in train_loader:\n    print(\"Data:\", data.shape, \"data type:\", data.dtype)\n    print(\"Label shape:\", label.shape, \"data type:\", label.dtype)\n    print(\"Cloud Mask shape:\", cloud_mask.shape, \"data type:\", cloud_mask.dtype)\n    break  # Remove or comment out this line to iterate through the entire dataset","metadata":{"id":"kXJHxgP5_xrC","outputId":"1b699b23-aaa0-4774-b875-57af74b694f9","tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def visualize_data(dataloader, num_rows=1):\n    # Fetch the first batch\n    for data, label, cloud_mask, tile_id in dataloader:\n        # Ensure num_rows does not exceed the batch size\n        num_rows = min(num_rows, data.shape[0])\n\n        fig, axes = plt.subplots(num_rows, 4, figsize=(20, 5 * num_rows))\n        # Ensure axes is always a 2D array for consistency\n        if num_rows == 1:\n            axes = axes.reshape(-1, 4)\n\n        for row in range(num_rows):\n            ax_rgb, ax_label, ax_cloud, ax_distance = axes[row]\n\n            # Display RGB Image\n            rgb = data[row][[2,3,4], :, :] \n            rgb = np.transpose(rgb.cpu().numpy(), (1, 2, 0))\n            rgb = (rgb - rgb.min()) / (rgb.max() - rgb.min())  # Normalize to [0, 1]\n            ax_rgb.imshow(rgb)\n            ax_rgb.set_title(f\"RGB Image - Sample {tile_id[row]}\")\n\n            # Display Label\n            label_np = label[row].squeeze().cpu().numpy()  # Ensure data is on CPU and squeezed\n            ax_label.imshow(label_np, cmap='gray')\n            ax_label.set_title(f\"Kelp Mask - Sample {tile_id[row]}\")\n\n            # Display Cloud Mask\n            cloud_mask_np = cloud_mask[row].squeeze().cpu().numpy()  # Ensure data is on CPU and squeezed\n            ax_cloud.imshow(cloud_mask_np, cmap='gray')\n            ax_cloud.set_title(f\"Cloud Mask - Sample {tile_id[row]}\")\n\n            # Display Distance Map\n            distance_map_np = data[row][5, :, :].squeeze().cpu().numpy()\n            ax_distance.imshow(distance_map_np, cmap='viridis')\n            ax_distance.set_title(f\"Distance Map - Sample {tile_id[row]}\")\n\n        plt.tight_layout()\n        plt.show()\n\n        # Break after visualizing the first batch\n        break\n\n# visualize_data(train_loader, num_rows=5)\n# print(\"***\"*100)\n# visualize_data(val_loader, num_rows=5)\n# print(\"***\"*100)\n# visualize_data(test_loader, num_rows=5)\n# print(\"***\"*100)\n# visualize_data(test_given_loader, num_rows=5)","metadata":{"id":"9KSjxfADpAba","outputId":"7dbab472-c75e-4898-b559-7117817bdd2e","tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Architectures\n\n\n","metadata":{"id":"_tLjFJo67M86"}},{"cell_type":"markdown","source":"### Segnet\nCode available on: https://github.com/vinceecws/SegNet_PyTorch/blob/master/Pavements/SegNet.py\n\n<img src=\"https://drive.google.com/uc?export=view&id=1-J5p-sMU_LSX1VbbXo6P---LL6-KYL5d\" width=\"750\"/>\n\n*Source: Badrinarayanan et al. (2017)*","metadata":{"id":"HjTQHIKLpC0S"}},{"cell_type":"code","source":"class SEGNET(nn.Module):\n\n    def __init__(self, in_chn, out_chn, BN_momentum):\n        super(SEGNET, self).__init__()\n\n        # Construct a string to represent the model configuration for identification\n        self.modelname = f\"SEGNET_in-chn={in_chn}_out-chn={out_chn}_BN_momentum={BN_momentum}\"\n\n        #SegNet Architecture\n        #Takes input of size in_chn = 3 (RGB images have 3 channels)\n        # Outputs size label_chn (N # of classes)\n\n        #ENCODING consists of 5 stages\n        #Stage 1, 2 has 2 layers of Convolution + Batch Normalization + Max Pool respectively\n        #Stage 3, 4, 5 has 3 layers of Convolution + Batch Normalization + Max Pool respectively\n\n        #General Max Pool 2D for ENCODING layers\n        #Pooling indices are stored for Upsampling in DECODING layers\n\n        self.in_chn = in_chn\n        self.out_chn = out_chn\n\n        self.MaxEn = nn.MaxPool2d(2, stride=2, return_indices=True)\n\n        self.ConvEn11 = nn.Conv2d(self.in_chn, 64, kernel_size=3, padding=1)\n        self.BNEn11 = nn.BatchNorm2d(64, momentum=BN_momentum)\n        self.ConvEn12 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n        self.BNEn12 = nn.BatchNorm2d(64, momentum=BN_momentum)\n\n        self.ConvEn21 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n        self.BNEn21 = nn.BatchNorm2d(128, momentum=BN_momentum)\n        self.ConvEn22 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n        self.BNEn22 = nn.BatchNorm2d(128, momentum=BN_momentum)\n\n        self.ConvEn31 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n        self.BNEn31 = nn.BatchNorm2d(256, momentum=BN_momentum)\n        self.ConvEn32 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n        self.BNEn32 = nn.BatchNorm2d(256, momentum=BN_momentum)\n        self.ConvEn33 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n        self.BNEn33 = nn.BatchNorm2d(256, momentum=BN_momentum)\n\n        self.ConvEn41 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n        self.BNEn41 = nn.BatchNorm2d(512, momentum=BN_momentum)\n        self.ConvEn42 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n        self.BNEn42 = nn.BatchNorm2d(512, momentum=BN_momentum)\n        self.ConvEn43 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n        self.BNEn43 = nn.BatchNorm2d(512, momentum=BN_momentum)\n\n        self.ConvEn51 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n        self.BNEn51 = nn.BatchNorm2d(512, momentum=BN_momentum)\n        self.ConvEn52 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n        self.BNEn52 = nn.BatchNorm2d(512, momentum=BN_momentum)\n        self.ConvEn53 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n        self.BNEn53 = nn.BatchNorm2d(512, momentum=BN_momentum)\n\n\n        #DECODING consists of 5 stages\n        #Each stage corresponds to their respective counterparts in ENCODING\n\n        #General Max Pool 2D/Upsampling for DECODING layers\n        self.MaxDe = nn.MaxUnpool2d(2, stride=2)\n\n        self.ConvDe53 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n        self.BNDe53 = nn.BatchNorm2d(512, momentum=BN_momentum)\n        self.ConvDe52 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n        self.BNDe52 = nn.BatchNorm2d(512, momentum=BN_momentum)\n        self.ConvDe51 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n        self.BNDe51 = nn.BatchNorm2d(512, momentum=BN_momentum)\n\n        self.ConvDe43 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n        self.BNDe43 = nn.BatchNorm2d(512, momentum=BN_momentum)\n        self.ConvDe42 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n        self.BNDe42 = nn.BatchNorm2d(512, momentum=BN_momentum)\n        self.ConvDe41 = nn.Conv2d(512, 256, kernel_size=3, padding=1)\n        self.BNDe41 = nn.BatchNorm2d(256, momentum=BN_momentum)\n\n        self.ConvDe33 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n        self.BNDe33 = nn.BatchNorm2d(256, momentum=BN_momentum)\n        self.ConvDe32 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n        self.BNDe32 = nn.BatchNorm2d(256, momentum=BN_momentum)\n        self.ConvDe31 = nn.Conv2d(256, 128, kernel_size=3, padding=1)\n        self.BNDe31 = nn.BatchNorm2d(128, momentum=BN_momentum)\n\n        self.ConvDe22 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n        self.BNDe22 = nn.BatchNorm2d(128, momentum=BN_momentum)\n        self.ConvDe21 = nn.Conv2d(128, 64, kernel_size=3, padding=1)\n        self.BNDe21 = nn.BatchNorm2d(64, momentum=BN_momentum)\n\n        self.ConvDe12 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n        self.BNDe12 = nn.BatchNorm2d(64, momentum=BN_momentum)\n        self.ConvDe11 = nn.Conv2d(64, self.out_chn, kernel_size=3, padding=1)\n        self.BNDe11 = nn.BatchNorm2d(self.out_chn, momentum=BN_momentum)\n\n    def forward(self, x):\n\n        #ENCODE LAYERS\n        #Stage 1\n        x = F.relu(self.BNEn11(self.ConvEn11(x)))\n        x = F.relu(self.BNEn12(self.ConvEn12(x)))\n        x, ind1 = self.MaxEn(x)\n        size1 = x.size()\n\n        #Stage 2\n        x = F.relu(self.BNEn21(self.ConvEn21(x)))\n        x = F.relu(self.BNEn22(self.ConvEn22(x)))\n        x, ind2 = self.MaxEn(x)\n        size2 = x.size()\n\n        #Stage 3\n        x = F.relu(self.BNEn31(self.ConvEn31(x)))\n        x = F.relu(self.BNEn32(self.ConvEn32(x)))\n        x = F.relu(self.BNEn33(self.ConvEn33(x)))\n        x, ind3 = self.MaxEn(x)\n        size3 = x.size()\n\n        #Stage 4\n        x = F.relu(self.BNEn41(self.ConvEn41(x)))\n        x = F.relu(self.BNEn42(self.ConvEn42(x)))\n        x = F.relu(self.BNEn43(self.ConvEn43(x)))\n        x, ind4 = self.MaxEn(x)\n        size4 = x.size()\n\n        #Stage 5\n        x = F.relu(self.BNEn51(self.ConvEn51(x)))\n        x = F.relu(self.BNEn52(self.ConvEn52(x)))\n        x = F.relu(self.BNEn53(self.ConvEn53(x)))\n        x, ind5 = self.MaxEn(x)\n        size5 = x.size()\n\n        #DECODE LAYERS\n        #Stage 5\n        x = self.MaxDe(x, ind5, output_size=size4)\n        x = F.relu(self.BNDe53(self.ConvDe53(x)))\n        x = F.relu(self.BNDe52(self.ConvDe52(x)))\n        x = F.relu(self.BNDe51(self.ConvDe51(x)))\n\n        #Stage 4\n        x = self.MaxDe(x, ind4, output_size=size3)\n        x = F.relu(self.BNDe43(self.ConvDe43(x)))\n        x = F.relu(self.BNDe42(self.ConvDe42(x)))\n        x = F.relu(self.BNDe41(self.ConvDe41(x)))\n\n        #Stage 3\n        x = self.MaxDe(x, ind3, output_size=size2)\n        x = F.relu(self.BNDe33(self.ConvDe33(x)))\n        x = F.relu(self.BNDe32(self.ConvDe32(x)))\n        x = F.relu(self.BNDe31(self.ConvDe31(x)))\n\n        #Stage 2\n        x = self.MaxDe(x, ind2, output_size=size1)\n        x = F.relu(self.BNDe22(self.ConvDe22(x)))\n        x = F.relu(self.BNDe21(self.ConvDe21(x)))\n\n        #Stage 1\n        x = self.MaxDe(x, ind1)\n        x = F.relu(self.BNDe12(self.ConvDe12(x)))\n        x = self.ConvDe11(x)\n\n        return x","metadata":{"id":"nVOwgEij7QKe","tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check sizes of Segnet\n\n# segnet_test = SEGNET(in_chn=6, out_chn=1, BN_momentum=0.5)\n\n# Create random input sizes\n# random_input_image = (1, 6, 350, 350)\n\n# Get a summary of the input and outputs of Unet\n# summary(model=segnet_test,\n#         input_size= random_input_image,\n#         col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n#         col_width=20,\n#         row_settings=[\"var_names\"])","metadata":{"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Unet Modified\n\nCode available on: https://github.com/4uiiurz1/pytorch-nested-unet/blob/master/archs.py","metadata":{}},{"cell_type":"code","source":"class VGGBlock(nn.Module):\n    def __init__(self, in_channels, middle_channels, out_channels):\n        super().__init__()\n        self.relu = nn.ReLU(inplace=True)\n        self.conv1 = nn.Conv2d(in_channels, middle_channels, 3, padding=1)\n        self.bn1 = nn.BatchNorm2d(middle_channels)\n        self.conv2 = nn.Conv2d(middle_channels, out_channels, 3, padding=1)\n        self.bn2 = nn.BatchNorm2d(out_channels)\n\n    def forward(self, x):\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        return out\n\n    \ndef pad_and_concat(x1, x2):\n    # Calculate size differences\n    diffY = x2.size()[2] - x1.size()[2]\n    diffX = x2.size()[3] - x1.size()[3]\n\n    # Padding to match the sizes\n    x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n                    diffY // 2, diffY - diffY // 2])\n\n    # Concatenating along the channel dimension\n    return torch.cat([x2, x1], dim=1)\n\nclass UNetModified(nn.Module):\n    def __init__(self, num_classes, input_channels=3, **kwargs):\n        super().__init__()\n        \n        # Construct a string to represent the model configuration for identification\n        self.modelname = f\"UNetMod_num-classes={num_classes}_input-channels={input_channels}\"\n\n        nb_filter = [64, 128, 256, 512, 1024]\n\n        self.pool = nn.MaxPool2d(2, 2)\n        self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n\n        self.conv0_0 = VGGBlock(input_channels, nb_filter[0], nb_filter[0])\n        self.conv1_0 = VGGBlock(nb_filter[0], nb_filter[1], nb_filter[1])\n        self.conv2_0 = VGGBlock(nb_filter[1], nb_filter[2], nb_filter[2])\n        self.conv3_0 = VGGBlock(nb_filter[2], nb_filter[3], nb_filter[3])\n        self.conv4_0 = VGGBlock(nb_filter[3], nb_filter[4], nb_filter[4])\n\n        self.conv3_1 = VGGBlock(nb_filter[3]+nb_filter[4], nb_filter[3], nb_filter[3])\n        self.conv2_2 = VGGBlock(nb_filter[2]+nb_filter[3], nb_filter[2], nb_filter[2])\n        self.conv1_3 = VGGBlock(nb_filter[1]+nb_filter[2], nb_filter[1], nb_filter[1])\n        self.conv0_4 = VGGBlock(nb_filter[0]+nb_filter[1], nb_filter[0], nb_filter[0])\n\n        self.final = nn.Conv2d(nb_filter[0], num_classes, kernel_size=1)\n        \n    def forward(self, input):\n        x0_0 = self.conv0_0(input)\n        x1_0 = self.conv1_0(self.pool(x0_0))\n        x2_0 = self.conv2_0(self.pool(x1_0))\n        x3_0 = self.conv3_0(self.pool(x2_0))\n        x4_0 = self.conv4_0(self.pool(x3_0))\n\n        # Use the pad_and_concat function for each concatenation step\n        x3_1 = self.conv3_1(pad_and_concat(self.up(x4_0), x3_0))\n        x2_2 = self.conv2_2(pad_and_concat(self.up(x3_1), x2_0))\n        x1_3 = self.conv1_3(pad_and_concat(self.up(x2_2), x1_0))\n        x0_4 = self.conv0_4(pad_and_concat(self.up(x1_3), x0_0))\n\n        output = self.final(x0_4)\n        return output\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# unet_modified_test = UNetModified(num_classes=1, input_channels=6)\n\n# # Create random input sizes\n# random_input_image = (1, 6, 350, 350)\n\n# # Get a summary of the input and outputs of Unet\n# summary(model=unet_modified_test,\n#         input_size= random_input_image,\n#         col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n#         col_width=20,\n#         row_settings=[\"var_names\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training and Testing","metadata":{"id":"FEoIwGyaDCcD"}},{"cell_type":"markdown","source":"### Loss Classes","metadata":{"id":"y_YerL6edwIb"}},{"cell_type":"code","source":"# Dice Loss. Source: https://github.com/Mr-TalhaIlyas/Loss-Functions-Package-Tensorflow-Keras-PyTorch\n# INPUTS: Logits\n\nclass DiceLoss(nn.Module):\n    def __init__(self):\n        super(DiceLoss, self).__init__()\n\n    def forward(self, logits, labels, smooth=1):\n        preds = torch.sigmoid(logits)  # Use torch.sigmoid to ensure compatibility\n\n        # Flatten label and prediction tensors\n        preds = preds.view(-1)\n        labels = labels.view(-1)\n\n\n        intersection = (preds * labels).sum()\n        dice = (2. * intersection + smooth) / (preds.sum() + labels.sum() + smooth)\n\n        return 1 - dice","metadata":{"id":"3wcg9VCAd07O","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Helper Functions","metadata":{"id":"ar7Z5qVx2DcQ"}},{"cell_type":"code","source":"def train_epoch(model, optimizer, loss_func, dataloader, device, use_distance_maps, use_dems, use_ndvi):\n    model.train()\n    running_loss = 0.0\n    running_iou = 0.0\n\n    # Determine indices to drop\n    drop_indices = []\n    if not use_distance_maps:\n        drop_indices.append(5)  # Index 5 is for distance maps\n    if not use_dems:\n        drop_indices.append(6)  # Index 6 is for DEMs\n    if not use_ndvi:\n        drop_indices.append(7)  # Index 7 is for NDVI\n\n    for data_batch, labels_batch, _, _ in dataloader:\n        # Exclude specified bands based on flags\n        if drop_indices:\n            data_batch = data_batch[:, [i for i in range(data_batch.shape[1]) if i not in drop_indices], :, :]\n\n        data_batch = data_batch.to(device)\n        labels_batch = labels_batch.to(device)\n\n        optimizer.zero_grad()\n        y_hat = model(data_batch)\n        loss = loss_func(y_hat, labels_batch)\n        iou = calculate_iou(y_hat, labels_batch)\n\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item() * data_batch.size(0)\n        running_iou += iou.item() * data_batch.size(0)\n\n    train_epoch_loss = running_loss / len(dataloader.dataset)\n    train_epoch_iou = running_iou / len(dataloader.dataset)\n    print(f'Train - Loss: {train_epoch_loss:.4f}, IoU: {train_epoch_iou:.4f}')\n\n    return train_epoch_loss, train_epoch_iou","metadata":{"id":"ldZXx-_2wbJ8","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def val_epoch(model, loss_func, dataloader, device, use_distance_maps, use_dems, use_ndvi):\n    model.eval()  # Set the model to evaluation mode\n    running_loss = 0.0\n    running_iou = 0.0\n\n    # Initialize lists to store true labels, predictions, and tile IDs\n    all_y_true, all_y_pred, all_tile_ids = [], [], []\n    \n    # Determine indices to drop\n    drop_indices = []\n    if not use_distance_maps:\n        drop_indices.append(5)  # Index 5 is for distance maps\n    if not use_dems:\n        drop_indices.append(6)  # Index 6 is for DEMs\n    if not use_ndvi:\n        drop_indices.append(7)  # Index 7 is for NDVI\n\n    with torch.no_grad():\n        for data_batch, labels_batch, _, tile_ids_batch in dataloader:\n            # Exclude specified bands based on flags\n            if drop_indices:\n                data_batch = data_batch[:, [i for i in range(data_batch.shape[1]) if i not in drop_indices], :, :]\n            \n            data_batch = data_batch.to(device)\n            labels_batch = labels_batch.to(device)\n\n            # Forward pass\n            y_hat = model(data_batch)\n            loss = loss_func(y_hat, labels_batch)\n            iou = calculate_iou(y_hat, labels_batch)\n\n            # Aggregate loss and IoU\n            running_loss += loss.item() * data_batch.size(0)\n            running_iou += iou.item() * data_batch.size(0)\n\n            # Store true labels and predictions\n            all_y_true.append(labels_batch.cpu().numpy())\n            all_y_pred.append(y_hat.cpu().numpy())\n            all_tile_ids.extend(tile_ids_batch)\n\n    # Calculate average loss and IoU for the epoch\n    val_epoch_loss = running_loss / len(dataloader.dataset)\n    val_epoch_iou = running_iou / len(dataloader.dataset)\n    print(f'Eval - Loss: {val_epoch_loss:.4f}, IoU: {val_epoch_iou:.4f}')\n\n    # Concatenate arrays of true labels and predictions\n    all_y_true = np.concatenate(all_y_true, axis=0)\n    all_y_pred = np.concatenate(all_y_pred, axis=0)\n\n    return val_epoch_loss, val_epoch_iou, all_y_true, all_y_pred, all_tile_ids\n","metadata":{"id":"T_4b1n7N1xTf","tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_checkpoint(checkpoints_path, model, optimizer, scheduler):\n    filepath = os.path.join(checkpoints_path, f'{model.modelname}.pth')\n    if os.path.isfile(filepath):\n        checkpoint = torch.load(filepath)\n        model.load_state_dict(checkpoint['model_state_dict'])\n        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n\n        # Load the scheduler state only if scheduler is not None and the state is saved in the checkpoint\n        if scheduler is not None and 'scheduler_state_dict' in checkpoint:\n            scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n\n        start_epoch = checkpoint['epoch'] + 1  # Continue from next epoch\n        min_val_loss = checkpoint['min_val_loss']\n        train_loss_history = checkpoint['train_loss_history']\n        val_loss_history = checkpoint['val_loss_history']\n        train_iou_history = checkpoint['train_iou_history']\n        val_iou_history = checkpoint['val_iou_history']\n        best_epoch = checkpoint.get('best_epoch', checkpoint['epoch'])  # Load best_epoch, default to current epoch if not found\n        print(f\"Loaded checkpoint '{filepath}' (epoch {checkpoint['epoch']})\")\n        return start_epoch, min_val_loss, train_loss_history, val_loss_history, train_iou_history, val_iou_history, best_epoch\n    else:\n        print(f\"No checkpoint found at '{filepath}', starting from scratch\")\n        return 1, np.inf, [], [], [], [], 0  # Include default best_epoch\n","metadata":{"id":"CkTJ-HBf7WwB","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_model(model, optimizer, loss_func, scheduler, train_dataloader, val_dataloader, device, num_epochs, checkpoints_path, use_distance_maps, use_dems, use_ndvi, use_checkpoint=False):\n    since = time.time()\n    model.to(device)\n\n    # Initialize training variables\n    start_epoch, min_val_loss, train_loss_history, val_loss_history, train_iou_history, val_iou_history = 1, np.inf, [], [], [], []\n    counter = 0\n    patience = 10\n    delta_p = 0.001\n    best_epoch = 0\n    best_checkpoint_path = os.path.join(checkpoints_path, f'{model.modelname}.pth')\n\n    # Load checkpoint if specified and exists\n    if use_checkpoint:\n        start_epoch, min_val_loss, train_loss_history, val_loss_history, train_iou_history, val_iou_history, best_epoch = load_checkpoint(checkpoints_path, model, optimizer, scheduler)\n\n    for epoch in range(start_epoch, num_epochs + 1):\n        print(f'Epoch {epoch}/{num_epochs}')\n        print('-' * 10)\n\n        # Training phase\n        train_epoch_loss, train_epoch_iou = train_epoch(model, optimizer, loss_func, train_dataloader, device, use_distance_maps, use_dems, use_ndvi)\n        train_loss_history.append(train_epoch_loss)\n        train_iou_history.append(train_epoch_iou)\n\n        # Validation phase\n        val_epoch_loss, val_epoch_iou, _, _, _ = val_epoch(model, loss_func, val_dataloader, device, use_distance_maps, use_dems, use_ndvi)\n        val_loss_history.append(val_epoch_loss)\n        val_iou_history.append(val_epoch_iou)\n\n        # Update the learning rate according to the scheduler, if it exists\n        if scheduler is not None:\n            scheduler.step()\n\n        # Checkpoint\n        if val_epoch_loss < min_val_loss - delta_p:\n            min_val_loss = val_epoch_loss\n            best_epoch = epoch\n            counter = 0\n            checkpoint = {\n                'epoch': epoch,\n                'model_state_dict': model.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict(),\n                'scheduler_state_dict': scheduler.state_dict() if scheduler is not None else None,  # Save the scheduler state only if it exists\n                'min_val_loss': min_val_loss,\n                'train_loss_history': train_loss_history,\n                'val_loss_history': val_loss_history,\n                'train_iou_history': train_iou_history,\n                'val_iou_history': val_iou_history,\n                'best_epoch': best_epoch\n            }\n            torch.save(checkpoint, best_checkpoint_path)\n            print(f\"Checkpoint saved at: {best_checkpoint_path}\")\n        else:\n            counter += 1\n\n        if counter == patience:\n            print(f'\\nEarly stopping after {patience} epochs without improvement.')\n            break\n\n    # Training Time Calculation\n    time_elapsed = time.time() - since\n    time_epoch = time_elapsed / num_epochs\n    time_epoch = f'{time_epoch // 60:.0f}m {time_epoch % 60:.0f}s'\n    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n\n    # Load best model weights before returning\n    if os.path.exists(best_checkpoint_path):\n        model.load_state_dict(torch.load(best_checkpoint_path)['model_state_dict'])\n\n    return model, time_epoch, train_loss_history, val_loss_history, train_iou_history, val_iou_history, best_epoch\n","metadata":{"id":"ElH1ZjR715yI","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# IOU\ndef calculate_iou(logits, labels, p_threshold=0.5, smooth=1e-6):\n    preds = (torch.sigmoid(logits) > p_threshold).float()\n\n    preds = preds.view(-1)\n    labels = labels.view(-1)\n\n    intersection = (preds * labels).sum()\n    union = preds.sum() + labels.sum() - intersection\n\n    IoU = (intersection + smooth) / (union + smooth)\n    \n    return IoU","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# DiceCoefficient\ndef calculate_dice_coefficient(logits, labels, p_threshold=0.5, smooth=1e-6):\n    preds = (torch.sigmoid(logits) > p_threshold).float()\n\n    preds = preds.view(-1)\n    labels = labels.view(-1)\n\n    intersection = (preds * labels).sum()\n    dice_coefficient = (2. * intersection + smooth) / (preds.sum() + labels.sum() + smooth)\n    \n    return dice_coefficient","metadata":{"id":"It6gNuqZYb_I","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_learning_curves(train_loss, val_loss, train_iou, val_iou, best_epoch):\n    # Create subplots with 1 row and 2 columns\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n\n    # Adjust epoch values for the training data to shift the curves to the left by 0.5\n    epochs_train = [x - 0.5 for x in range(1, len(train_loss) + 1)]\n    epochs_val = range(1, len(val_loss) + 1)  # Validation epochs remain unchanged\n\n    # Plot Training and Validation Loss on the first axis\n    ax1.set_title('Training and Validation Loss')\n    ax1.plot(epochs_train, train_loss, label=\"Train Loss\")  # Use adjusted epochs for training data\n    ax1.plot(epochs_val, val_loss, label=\"Validation Loss\")\n    ax1.axvline(x=best_epoch, color='grey', linestyle='--', label=f'Best Epoch: {best_epoch}')\n    ax1.set_xlabel('Epochs')\n    ax1.set_ylabel('Loss')\n    ax1.legend()\n\n    # Plot Training and Validation IoU on the second axis\n    ax2.set_title('Training and Validation IoU')\n    ax2.plot(epochs_train, train_iou, label=\"Train IoU\")  # Use adjusted epochs for training data\n    ax2.plot(epochs_val, val_iou, label=\"Validation IoU\")\n    ax2.axvline(x=best_epoch, color='grey', linestyle='--', label=f'Best Epoch: {best_epoch}')\n    ax2.set_xlabel('Epochs')\n    ax2.set_ylabel('IoU')\n    ax2.legend()\n\n    # Adjust layout and show the plot\n    plt.tight_layout()\n    plt.show()","metadata":{"id":"2Myz-UyW2P2U","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def evaluate_segmentation_performance_global(y_true, y_pred):\n    # Convert NumPy arrays back to PyTorch tensors\n    y_true = torch.tensor(y_true, dtype=torch.float32)\n    y_pred = torch.tensor(y_pred, dtype=torch.float32)\n\n    # Calculate metrics using tensors\n    iou = calculate_iou(y_pred, y_true)\n    dice_coefficient = calculate_dice_coefficient(y_pred, y_true)\n\n    print(f\"Global Metrics in Testing Set: \")\n    print(f\"IoU: {iou:.4f}\")\n    print(f\"Dice Coefficient: {dice_coefficient:.4f}\")\n","metadata":{"id":"_Y5rg33ytO4M","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def evaluate_and_plot_performance_individual(y_true, y_pred, tile_ids, num_extremes=5, p_threshold=0.5):\n    num_images = y_true.shape[0]\n    dice_coefficients = []\n\n    y_true = torch.tensor(y_true, dtype=torch.float32)\n    y_pred = torch.tensor(y_pred, dtype=torch.float32)\n\n    # Calculate Dice Coefficients for each image\n    for i in range(num_images):\n        dice_coef = calculate_dice_coefficient(y_pred[i], y_true[i])\n        dice_coefficients.append(dice_coef)\n    \n    # Sorting indices by Dice Coefficient\n    sorted_indices = sorted(range(num_images), key=lambda x: dice_coefficients[x])\n    extremes_indices = sorted_indices[:num_extremes] + sorted_indices[-num_extremes:]\n\n    # Plotting the distribution of Dice coefficients\n    fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n    ax.hist(dice_coefficients, bins=40, color='skyblue')\n    ax.set_title('Distribution of Individual Dice Coefficients')\n    ax.set_xlabel('Dice Coefficient')\n    ax.set_ylabel('Frequency')\n    plt.show()\n\n    # Plotting the extremes\n    y_pred = (torch.sigmoid(y_pred) > p_threshold).float()\n    fig, axs = plt.subplots(len(extremes_indices), 2, figsize=(10, 5 * len(extremes_indices)))\n\n    for i, index in enumerate(extremes_indices):\n        # Prediction\n        axs[i, 0].imshow(y_pred[index].squeeze().cpu().numpy(), cmap='viridis')\n        axs[i, 0].set_title(f'Prediction (Binary) - Tile {tile_ids[index]} - Dice: {dice_coefficients[index]:.4f}')\n        axs[i, 0].axis('off')\n\n        # Ground Truth\n        axs[i, 1].imshow(y_true[index].squeeze().cpu().numpy(), cmap='viridis')\n        axs[i, 1].set_title(f'Ground Truth - Tile {tile_ids[index]}')\n        axs[i, 1].axis('off')\n\n    plt.tight_layout()\n    plt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def postprocess_and_export_predictions(model, test_given_loader, device, predictions_path, use_distance_maps, use_dems, use_ndvi, postprocess=True, threshold=0, p_threshold=0.5, all_means=all_means, all_stds=all_stds):\n    model.to(device)\n    model.eval()\n\n    # Ensure the predictions directory exists\n    if not os.path.exists(predictions_path):\n        os.makedirs(predictions_path)\n\n    # Determine indices to drop\n    drop_indices = []\n    if not use_distance_maps:\n        drop_indices.append(5)  # Index 5 is for distance maps\n    if not use_dems:\n        drop_indices.append(6)  # Index 6 is for DEMs\n    if not use_ndvi:\n        drop_indices.append(7)  # Index 7 is for NDVI\n\n    with torch.no_grad():\n        for data_batch, _, _, tile_ids_batch in tqdm(test_given_loader, desc='Processing', leave=True):\n            # Extract the DEM band\n            dem_band_batch = data_batch[:, 6:7, :, :]\n\n            # Unnormalize the DEM band using the mean and standard deviation\n            if all_means is not None and all_stds is not None:\n                dem_mean = all_means[6]\n                dem_std = all_stds[6]\n                dem_band_batch = (dem_band_batch * dem_std) + dem_mean\n\n            # Extract the red band for additional verification\n            red_band_batch = data_batch[:, 2:3, :, :]\n\n            # Drop bands not included in training\n            if drop_indices:\n                data_batch = data_batch[:, [i for i in range(data_batch.shape[1]) if i not in drop_indices], :, :]\n\n            data_batch = data_batch.to(device)\n            outputs = model(data_batch)\n            predictions = torch.sigmoid(outputs) > p_threshold\n\n            for idx, prediction in enumerate(predictions):\n                # Verification based on the red band\n                invalid_mask = red_band_batch[idx] == -32768\n\n                if postprocess:\n                    # Use the previously extracted DEM band for post-processing\n                    dem_band = dem_band_batch[idx].to(device)\n\n                    # Create a mask where DEM values are greater than the threshold\n                    mask = dem_band > threshold\n\n                    # Adjust prediction tensor shape if necessary\n                    if prediction.dim() > mask.dim():\n                        prediction = prediction.squeeze(0)\n\n                    prediction[mask] = 0  # Apply the mask to the prediction\n\n                # Set invalid pixels to 0 based on the red band verification\n                prediction[invalid_mask] = 0\n\n                # Prepare the prediction for saving\n                prediction = prediction.cpu().squeeze().numpy().astype(np.uint8)\n\n                # Save the prediction as a TIFF file\n                tiff_filename = os.path.join(predictions_path, f'{tile_ids_batch[idx]}_kelp.tif')\n                Image.fromarray(prediction).save(tiff_filename, format='TIFF')\n\n    # Create an archive of all the prediction TIFF files\n    archive_name = 'predictions.tar.gz'\n    with tarfile.open(os.path.join(predictions_path, archive_name), 'w:gz') as tar:\n        for file in os.listdir(predictions_path):\n            if file.endswith('.tif'):\n                tar.add(os.path.join(predictions_path, file), arcname=file)\n\n    print(f'All predictions are saved and archived in {predictions_path}, archive name: {archive_name}')","metadata":{"id":"Fl5vjWIULpaQ","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Model Configurations","metadata":{"id":"d7VkxKwZ2a0p"}},{"cell_type":"code","source":"# Flags to indicate whether to include specific bands for training\nuse_distance_maps = True  # Include distance maps\nuse_dems = False  # Include DEMs band\nuse_ndvi = False  # Include NDVI band\n\n# Base number of bands without additional features\nbase_bands = 5\n\n# Calculate total number of bands by adding 1 for each additional feature used\nn_bands = base_bands + use_distance_maps + use_dems + use_ndvi","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Instantiate Model\n#segnet = SEGNET(in_chn = n_bands, out_chn = 1, BN_momentum = 0.5)\nunet_modified = UNetModified(num_classes=1, input_channels=n_bands)","metadata":{"id":"cmj2PgrzEijA","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check if CUDA (GPU) is available\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\n# Print the selected device\nprint(device)","metadata":{"id":"yaYRpytE2toN","outputId":"4d0d4605-c9f3-4997-fa31-cfc3800f2e88","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Set Up Loss Function\nloss_func = DiceLoss()","metadata":{"id":"jsp4j5QEgQVj","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Set Up Optimizer\nlr = 0.0001\nweight_decay = 1e-7\noptimizer = torch.optim.Adam(unet_modified.parameters(), lr = lr, weight_decay = weight_decay)","metadata":{"id":"PO3kUoffEXHy","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Initialize the Exponential Decay Learning Rate Scheduler\ngamma = 0.95\nscheduler = None","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Set Up the number of epochs\nnum_epochs = 80","metadata":{"id":"g5gKSWRtfAzM","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Training","metadata":{"id":"tUYeQzUSkSWt"}},{"cell_type":"code","source":"use_checkpoint = False  # Set to False if you want to start training from scratch\nmodel, time_epoch, train_loss_history, val_loss_history, train_iou_history, val_iou_history, best_epoch = train_model(\n    unet_modified, optimizer, loss_func, scheduler, train_loader, val_loader, device, num_epochs, checkpoints, use_distance_maps, use_dems, use_ndvi, use_checkpoint)","metadata":{"id":"J-nRiKVIkYx2","outputId":"6cb7f2c3-921d-4034-f1ac-8eb2dacf51b2","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot model learning curves\nplot_learning_curves(train_loss_history, val_loss_history, train_iou_history, val_iou_history, best_epoch)","metadata":{"id":"Ad7WaEiY9jhK","outputId":"300ef1b5-2900-45a1-c27a-dcb8b1d089f0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Testing","metadata":{"id":"EvYzmkc49zaF"}},{"cell_type":"code","source":"# Evaluate model on the testing set: Loss and IOU\nval_epoch_loss, val_epoch_iou, y_true, y_pred , tile_ids = val_epoch(model, loss_func, test_loader, device, use_distance_maps, use_dems, use_ndvi)","metadata":{"id":"z30taeJeqVs7","outputId":"e9d33f16-7c6d-41aa-e5c0-9cffa71eacd4","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Metrics GLOBAL\nevaluate_segmentation_performance_global(y_true, y_pred)","metadata":{"id":"xNcpXLrutSYl","outputId":"de68533e-7cda-4a60-88b1-b78d68b3fd78","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Metrics INDIVIDUAL\nevaluate_and_plot_performance_individual(y_true, y_pred, tile_ids, num_extremes=5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Submission Folder","metadata":{"id":"fDbuGUChbVNo"}},{"cell_type":"code","source":"# Call the function with the modified parameters\npostprocess_and_export_predictions(model, test_loader, device, predictions, use_distance_maps, use_dems, use_ndvi, postprocess=True, threshold=0)","metadata":{"id":"TF8E1FzcoaO5","outputId":"1f76830f-8df4-46d2-c3cc-98690c8eebf5","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## References\n\n* Badrinarayanan, V., Kendall, A., & Cipolla, R. (2017). Segnet: A deep convolutional encoder-decoder architecture for image segmentation. IEEE transactions on pattern analysis and machine intelligence, 39(12), 2481-2495.\n* Ronneberger, O., Fischer, P., & Brox, T. (2015). U-net: Convolutional networks for biomedical image segmentation. In Medical Image Computing and Computer-Assisted InterventionMICCAI 2015: 18th International Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III 18 (pp. 234-241). Springer International Publishing.","metadata":{"id":"fv-5T0qg9vIK"}}]}